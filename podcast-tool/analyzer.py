#!/usr/bin/env python3
"""
æ’­å®¢å†…å®¹åˆ†æå™¨ â€” ç”¨ Claude API å°†è½¬å½•æ–‡æœ¬è½¬ä¸ºç»“æ„åŒ– JSON
è¾“å…¥ï¼šè½¬å½•æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆ.txtï¼‰
è¾“å‡ºï¼šç¬¦åˆé€šç”¨æ•°æ®æ¨¡å‹çš„ episode.json
"""

import os
import sys
import json
import re

try:
    from openai import OpenAI
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–ï¼Œè¯·è¿è¡Œï¼špip install openai")
    sys.exit(1)

# é€šä¹‰åƒé—® API é…ç½®
QWEN_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
QWEN_MODEL = "qwen-plus"

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ’­å®¢å†…å®¹åˆ†æå¸ˆï¼Œæ“…é•¿å°†æ’­å®¢è½¬å½•æ–‡æœ¬æç‚¼ä¸ºç»“æ„åŒ–å†…å®¹ã€‚

ä½ çš„ä»»åŠ¡ï¼šåˆ†ææ’­å®¢è½¬å½•æ–‡æœ¬ï¼Œè¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼æ•°æ®ã€‚

## è¾“å‡ºæ ¼å¼

å¿…é¡»è¾“å‡ºåˆæ³•çš„ JSONï¼Œä¸åŠ ä»»ä½•å¤šä½™æ–‡å­—ï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ç»“æ„ï¼š

```json
{
  "meta": {
    "podcast_name": "æ’­å®¢åç§°",
    "episode_number": 59,
    "title": "æœ¬æœŸæ ‡é¢˜",
    "subtitle": "ä¸€å¥è¯æè¿°æœ¬æœŸä¸»é¢˜",
    "total_duration_sec": 5400,
    "language": "zh"
  },
  "participants": [
    {
      "id": "host1",
      "name": "ä¸»æŒäººåå­—",
      "role": "host",
      "bio": "ç®€çŸ­ä»‹ç»ï¼ˆå¯é€‰ï¼‰"
    }
  ],
  "featured_work": {
    "type": "book",
    "title": "ä¹¦åï¼ˆè‹¥æœ¬æœŸè®¨è®ºäº†æŸæœ¬ä¹¦/ç”µå½±ï¼‰",
    "author": "ä½œè€…å"
  },
  "sections": [
    {
      "id": "intro",
      "title": "ç« èŠ‚ä¸»æ ‡é¢˜",
      "subtitle": "ç« èŠ‚å‰¯æ ‡é¢˜",
      "start_sec": 0,
      "end_sec": 480,
      "is_ad": false,
      "key_points": ["æ ¸å¿ƒè§‚ç‚¹1", "æ ¸å¿ƒè§‚ç‚¹2", "æ ¸å¿ƒè§‚ç‚¹3"],
      "quotes": ["é‡‘å¥1", "é‡‘å¥2"],
      "stories": [
        {
          "narrator_id": "host1",
          "text": "ä¸ªäººæ•…äº‹æˆ–æ¡ˆä¾‹æè¿°"
        }
      ],
      "key_points_grouped": [{"label": "åˆ†ç»„å", "visual_type": "list", "points": [{"text": "è¦ç‚¹", "detail": "è¡¥å……"}]}],
      "diagram": {
        "type": "flow|comparison|icon-list|slope|layers",
        "title": "å›¾è¡¨æ ‡é¢˜",
        "description": "å¯é€‰è¯´æ˜",
        "// flow ç±»å‹éœ€è¦": "steps: [{label, desc}]",
        "// comparison ç±»å‹éœ€è¦": "left: {label}, right: {label}, entries: [{left, right}]",
        "// icon-list ç±»å‹éœ€è¦": "entries: [{icon, label, desc}]",
        "// slope ç±»å‹éœ€è¦": "elements: [{label, level}]ï¼Œlevel ä¸º low/barrier/high",
        "// layers ç±»å‹éœ€è¦": "layers: [{label, desc, depth}]ï¼Œdepth ä¸º 1/2/3"
      },
      "section_context": "æœ¬ç« åœ¨å…¨é›†é€»è¾‘ä¸­çš„ä½ç½®ï¼ˆä¸€å¥è¯ï¼‰"
    }
  ],
  "core_quotes": [
    "ç²¾é€‰é‡‘å¥1ï¼ˆæœ€æœ‰åŠ›é‡çš„5-10æ¡ï¼Œè·¨ç« èŠ‚æå–ï¼‰",
    "ç²¾é€‰é‡‘å¥2"
  ],
  "recommendations": [
    {
      "type": "book",
      "title": "ä¹¦å",
      "author": "ä½œè€…",
      "quote": "æ¨èç†ç”±"
    }
  ],
  "quiz": {
    "intro": "ä¸æœ¬æœŸä¸»é¢˜ç›¸å…³çš„è‡ªæµ‹å¼•å¯¼è¯­",
    "questions": [
      {
        "id": "q1",
        "text": "é—®é¢˜æ–‡å­—",
        "type": "choice",
        "options": [
          {"label": "é€‰é¡¹A", "score": 0},
          {"label": "é€‰é¡¹B", "score": 1},
          {"label": "é€‰é¡¹C", "score": 2},
          {"label": "é€‰é¡¹D", "score": 3}
        ]
      }
    ],
    "result_levels": [
      {
        "max_avg_score": 1.0,
        "level_label": "æ–°æ‰‹çº§",
        "description": "ç»“æœæè¿°"
      },
      {
        "max_avg_score": 2.0,
        "level_label": "è¿›é˜¶çº§",
        "description": "ç»“æœæè¿°"
      },
      {
        "max_avg_score": 3.0,
        "level_label": "é«˜æ‰‹çº§",
        "description": "ç»“æœæè¿°"
      }
    ]
  },
  "content_overview": {
    "one_sentence_summary": "ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒä¸»æ—¨ï¼ˆ15-30å­—ï¼‰",
    "content_blocks": [
      {"id": "block-1", "title": "ç»„å—æ ‡é¢˜", "summary": "ç»„å—æ¦‚è¦", "section_ids": ["section-id"], "icon": "ğŸ¯"}
    ],
    "block_connections": [
      {"from": "block-1", "to": "block-2", "relation": "å»¶ä¼¸", "description": "é€»è¾‘å…³ç³»è¯´æ˜"}
    ]
  },
  "arguments": [
    {"id": "arg-1", "claim": "è§‚ç‚¹é™ˆè¿°", "evidence_type": "ä¸ªäººç»å†", "evidence": "è®ºæ®æ¦‚è¿°", "source_section_id": "section-id", "strength": "strong"}
  ],
  "key_concepts": [
    {"id": "concept-1", "term": "æ¦‚å¿µåç§°", "definition": "ç®€æ´å®šä¹‰", "explanation": "æ’­å®¢ä¸­å¦‚ä½•é˜è¿°", "examples": ["å…·ä½“ä¾‹å­"], "related_concepts": ["concept-2"], "source_section_id": "section-id"}
  ],
  "extended_reading": [
    {"id": "ext-1", "topic": "å»¶ä¼¸ä¸»é¢˜", "context": "è¯é¢˜èƒŒæ™¯", "deep_dive": "å»¶ä¼¸è§£è¯»", "related_concept_ids": ["concept-1"], "further_resources": "æ¨èæ–¹å‘"}
  ],
  "mind_map": {
    "central_theme": "æ ¸å¿ƒä¸»é¢˜",
    "nodes": [
      {"id": "node-1", "label": "èŠ‚ç‚¹æ ‡ç­¾", "type": "theme", "parent_id": null, "detail": "èŠ‚ç‚¹è¯´æ˜"},
      {"id": "node-1-1", "label": "å­èŠ‚ç‚¹", "type": "concept", "parent_id": "node-1", "detail": "å­èŠ‚ç‚¹è¯´æ˜"}
    ]
  }
}
```

## æ³¨æ„äº‹é¡¹

1. **æ—¶é—´æˆ³**ï¼šè½¬å½•æ–‡æœ¬ä¸­æœ‰ [MM:SS - MM:SS] æ ¼å¼ï¼Œå°†å…¶è½¬ä¸ºç§’æ•°ï¼ˆå¦‚ [08:08] = 488ç§’ï¼‰
2. **ç« èŠ‚åˆ’åˆ†**ï¼šæŒ‰è¯é¢˜è½¬æ¢è‡ªç„¶åˆ’åˆ†ï¼Œé€šå¸¸ 6-10 ä¸ªç« èŠ‚ï¼Œå¹¿å‘Šæ®µæ ‡è®° is_ad: true
3. **å‚ä¸è€… ID**ï¼šä½¿ç”¨ç®€æ´ IDï¼ˆå¦‚ "host", "guest", "via", æˆ–è¯´è¯äººåå­—çš„æ‹¼éŸ³ç¼©å†™ï¼‰
4. **key_points**ï¼šæ¯ç« èŠ‚ 3-6 æ¡ï¼Œç®€æ´æœ‰åŠ›
5. **core_quotes**ï¼šå…¨é›†æœ€ç²¾å½©çš„ 5-10 å¥è¯ï¼Œå¯ç›´æ¥ä½œä¸ºåˆ†äº«é‡‘å¥
6. **quiz**ï¼š5 é“ä¸æœ¬æœŸä¸»é¢˜ç´§å¯†ç›¸å…³çš„è‡ªæµ‹é¢˜ï¼Œè®©å¬ä¼—åæ€è‡ªèº«
7. **featured_work**ï¼šä»…å½“æœ¬æœŸæ˜ç¡®å›´ç»•æŸä¹¦/ç”µå½±/ä½œå“å±•å¼€æ—¶å¡«å†™ï¼Œå¦åˆ™çœç•¥è¯¥å­—æ®µ
8. **recommendations**ï¼šæ”¶é›†èŠ‚ç›®ä¸­æåˆ°çš„ä¹¦å•/å½±å•/æ’­å®¢æ¨èï¼Œå¯ä¸ºç©ºæ•°ç»„
9. **content_overview**ï¼šå°†ç« èŠ‚å½’çº³ä¸º 3-5 ä¸ªç»„å—ï¼Œæè¿°ç»„å—é—´é€»è¾‘å…³ç³»ï¼ˆå› æœ/é€’è¿›/å¯¹æ¯”/å»¶ä¼¸ï¼‰
10. **arguments**ï¼šæå– 8-12 ä¸ªæ ¸å¿ƒè§‚ç‚¹ï¼Œevidence_type ä¸ºï¼šä¸ªäººç»å†/ç±»æ¯”/å¼•ç”¨/æ•°æ®/é€»è¾‘æ¨æ¼”/æ•…äº‹ï¼Œstrength ä¸º strong/moderate/anecdotal
11. **key_concepts**ï¼šæå– 6-10 ä¸ªåå¤å‡ºç°çš„å…³é”®æ¦‚å¿µï¼Œå«å®šä¹‰ã€æ’­å®¢ä¸­çš„é˜è¿°å’Œå…·ä½“ä¾‹å­
12. **extended_reading**ï¼šå»¶ä¼¸ 4-6 ä¸ªè¯é¢˜æ–¹å‘ï¼Œdeep_dive å¯è¶…å‡ºæ’­å®¢å†…å®¹
13. **mind_map**ï¼š2-3 å±‚æ ‘çŠ¶ç»“æ„ï¼Œtype ä¸º theme/concept/argument/exampleï¼Œä¸€çº§ 3-5 ä¸ªèŠ‚ç‚¹
14. **key_points_grouped**ï¼šå°† key_points æŒ‰é€»è¾‘åˆ†ç»„ï¼ˆ2-4ç»„ï¼‰ï¼Œæ¯ç»„æœ‰ label å’Œ pointsã€‚**text å¿…é¡»æ˜¯å®Œæ•´è§‚ç‚¹å¥ï¼ˆ15-40å­—ï¼‰ï¼Œç¦æ­¢åªå†™å…³é”®è¯ï¼detail å¿…é¡»åŒ…å«å…·ä½“è®ºæ®ã€æ•°æ®æˆ–åŸæ–‡å¼•è¿°ï¼ˆ20-80å­—ï¼‰ã€‚** è¯»è€…ä»…é€šè¿‡ key_points_grouped å°±èƒ½ç†è§£æœ¬ç«  80% çš„æ ¸å¿ƒå†…å®¹ã€‚visual_type å¯é€‰å€¼ï¼šlistï¼ˆé»˜è®¤ï¼‰ã€comparisonï¼ˆå¯¹æ¯”ï¼‰ã€flowï¼ˆæµç¨‹ï¼‰ã€icon-gridï¼ˆå›¾æ ‡ç½‘æ ¼ï¼‰
15. **diagram**ï¼šå½“ç« èŠ‚å†…å®¹é€‚åˆç”¨å›¾è¡¨è¾…åŠ©ç†è§£æ—¶æ·»åŠ ï¼Œæ¯æœŸé€šå¸¸ 3-5 ä¸ªã€‚**å¿…é¡»åŒ…å«å®Œæ•´æ•°æ®å­—æ®µï¼Œä¸èƒ½åªå†™ type/title/descriptionï¼**å„ç±»å‹å¿…éœ€å­—æ®µï¼š
    - flowï¼š`steps: [{label: "æ­¥éª¤å", desc: "è¯´æ˜"}]`ï¼ˆ3-6æ­¥ï¼‰
    - comparisonï¼š`left: {label: "å·¦ä¾§æ ‡ç­¾"}, right: {label: "å³ä¾§æ ‡ç­¾"}, entries: [{left: "å·¦ä¾§å†…å®¹", right: "å³ä¾§å†…å®¹"}]`ï¼ˆ3-5è¡Œï¼‰
    - icon-listï¼š`entries: [{icon: "emoji", label: "æ ‡ç­¾", desc: "è¯´æ˜"}]`ï¼ˆ4-8é¡¹ï¼‰
    - slopeï¼š`elements: [{label: "æ ‡ç­¾", level: "low|barrier|high"}]`ï¼ˆ3-5é¡¹ï¼‰
    - layersï¼š`layers: [{label: "æ ‡ç­¾", desc: "è¯´æ˜", depth: 1|2|3}]`ï¼ˆ3å±‚ï¼‰
16. **è¾“å‡ºçº¯ JSON**ï¼šä¸è¦åŠ  ```json ä»£ç å—æ ‡è®°ï¼Œä¸è¦åŠ è§£é‡Šæ–‡å­—
"""


def analyze_transcript(transcript_path: str, output_path: str = None, metadata: dict = None) -> dict:
    """
    ä½¿ç”¨ Claude åˆ†æè½¬å½•æ–‡æœ¬ï¼Œç”Ÿæˆç»“æ„åŒ– JSON

    Args:
        transcript_path: è½¬å½•æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼ˆ.txtï¼‰
        output_path: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„ï¼ŒNone åˆ™è‡ªåŠ¨æ¨æ–­
        metadata: å¯é€‰çš„å…ƒæ•°æ® dictï¼ˆpodcast_name, cover_url ç­‰ï¼‰

    Returns:
        dict: ç»“æ„åŒ–çš„ episode æ•°æ®
    """
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        raise RuntimeError("âŒ DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®ï¼ˆé€šä¹‰åƒé—® API Keyï¼‰")

    if not os.path.exists(transcript_path):
        raise FileNotFoundError(f"è½¬å½•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{transcript_path}")

    print(f"ğŸ“– è¯»å–è½¬å½•æ–‡æœ¬ï¼š{transcript_path}")
    with open(transcript_path, "r", encoding="utf-8") as f:
        transcript_text = f.read()

    char_count = len(transcript_text)
    print(f"   å­—ç¬¦æ•°ï¼š{char_count:,}")

    # æ„å»ºç”¨æˆ·æ¶ˆæ¯
    meta_hint = ""
    if metadata:
        meta_hint = f"""
## å·²çŸ¥å…ƒæ•°æ®ï¼ˆç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€ä»æ–‡æœ¬ä¸­æ¨æ–­ï¼‰
- æ’­å®¢åç§°ï¼š{metadata.get('podcast_name', 'æœªçŸ¥')}
- æœ¬æœŸæ ‡é¢˜ï¼š{metadata.get('title', 'æœªçŸ¥')}
- ç®€ä»‹ï¼š{metadata.get('description', 'æ— ')[:200]}

"""

    user_message = f"""{meta_hint}## è½¬å½•æ–‡æœ¬

{transcript_text}

è¯·åˆ†æä»¥ä¸Šè½¬å½•æ–‡æœ¬ï¼Œè¾“å‡ºç¬¦åˆè¦æ±‚çš„ JSON ç»“æ„ã€‚"""

    print(f"ğŸ¤– è°ƒç”¨é€šä¹‰åƒé—®åˆ†æä¸­ï¼ˆ{QWEN_MODEL}ï¼‰...")
    print(f"   é¢„è®¡è€—æ—¶ï¼š30-120 ç§’...")

    client = OpenAI(api_key=api_key, base_url=QWEN_BASE_URL)

    try:
        response = client.chat.completions.create(
            model=QWEN_MODEL,
            max_tokens=16384,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_message},
            ],
        )
    except Exception as e:
        raise RuntimeError(f"é€šä¹‰åƒé—® API è°ƒç”¨å¤±è´¥ï¼š{e}")

    raw_output = response.choices[0].message.content.strip()

    # è§£æ JSONï¼ˆæœ‰æ—¶ Claude ä¼šåŠ  ```json ä»£ç å—ï¼‰
    episode_data = _parse_json_output(raw_output)

    # æ³¨å…¥å°é¢ URLï¼ˆClaude æ— æ³•ä»æ–‡æœ¬ä¸­è·å–ï¼‰
    if metadata:
        if "meta" not in episode_data:
            episode_data["meta"] = {}
        if metadata.get("cover_url"):
            episode_data["meta"]["cover_url"] = metadata["cover_url"]
        if metadata.get("audio_url"):
            episode_data["meta"]["audio_url"] = metadata["audio_url"]
        if metadata.get("source_url"):
            episode_data["meta"]["platform_links"] = {
                "xiaoyuzhou": metadata["source_url"]
            }

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if not output_path:
        base = os.path.splitext(transcript_path)[0]
        output_path = base.replace("transcript", "episode") + ".json"
        if output_path == transcript_path:
            output_path = os.path.splitext(transcript_path)[0] + "_episode.json"

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(episode_data, f, ensure_ascii=False, indent=2)

    sections_count = len(episode_data.get("sections", []))
    quotes_count = len(episode_data.get("core_quotes", []))
    print(f"âœ… å†…å®¹åˆ†æå®Œæˆ")
    print(f"   ç« èŠ‚æ•°ï¼š{sections_count}")
    print(f"   ç²¾é€‰é‡‘å¥ï¼š{quotes_count} æ¡")
    print(f"   è¾“å‡ºæ–‡ä»¶ï¼š{output_path}")

    return episode_data


def _parse_json_output(raw: str) -> dict:
    """è§£æ Claude çš„ JSON è¾“å‡ºï¼Œå…¼å®¹å¸¦ä»£ç å—çš„æƒ…å†µ"""
    # å»é™¤ ```json ... ``` åŒ…è£¹
    cleaned = re.sub(r"^```(?:json)?\s*\n?", "", raw.strip(), flags=re.MULTILINE)
    cleaned = re.sub(r"\n?```\s*$", "", cleaned.strip(), flags=re.MULTILINE)
    cleaned = cleaned.strip()

    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
        start = cleaned.find("{")
        end = cleaned.rfind("}") + 1
        if start >= 0 and end > start:
            try:
                return json.loads(cleaned[start:end])
            except json.JSONDecodeError:
                pass
        raise ValueError(f"æ— æ³•è§£æ Claude è¾“å‡ºä¸º JSONï¼š{e}\nåŸå§‹è¾“å‡ºå‰ 500 å­—ç¬¦ï¼š\n{raw[:500]}")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼špython3 analyzer.py <è½¬å½•æ–‡æœ¬è·¯å¾„> [è¾“å‡ºJSONè·¯å¾„]")
        print("ç¤ºä¾‹ï¼špython3 analyzer.py ./output/abc123/transcript.txt")
        sys.exit(1)

    transcript_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else None

    result = analyze_transcript(transcript_path, output_path)
    print(f"\nåˆ†æç»“æœæ‘˜è¦ï¼š{result.get('meta', {}).get('title', 'æ— æ ‡é¢˜')}")
